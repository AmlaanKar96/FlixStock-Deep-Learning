# -*- coding: utf-8 -*-
"""Multilabel_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qpwJKtdzfO7vTLj0rxbHRQ_rmvrwcq0
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt
from PIL import Image
import os
import pickle
import glob
import random
import itertools
import copy
import math
from skimage import color
from sklearn.metrics import accuracy_score
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix
import seaborn as sns
import csv
import pandas as pd
from sklearn.model_selection import train_test_split

parser = argparse.ArgumentParser(description='PyTorch Training')
parser.add_argument('data', metavar='DIR', default='/content/drive/My Drive/Colab Notebooks/FlixStock/classification-assignment.zip (Unzipped Files)/classification-assignment/images/')
parser.add_argument('attributes', default='/content/drive/My Drive/Colab Notebooks/FlixStock/classification-assignment.zip (Unzipped Files)/classification-assignment/attributes.csv')
args = parser.parse_args()

filenames_train=glob.glob(args.data+'*')
filenames_train=sorted(filenames_train)
filenames_train=sorted(filenames_train,key=len)

len(filenames_train)

df=pd.read_csv(args.attributes)
df

df=df.values
len(df)

df_dict,dtset,labels,dtset_dict={},[],[],{}
for i in df:
    df_dict[i[0]]=i
for i in filenames_train:
    ret=i.split('/')[-1]
    try:
        ret1=df_dict[ret]
        try:
            ret1=int(df_dict[ret][1])
            ret2=int(df_dict[ret][2])
            ret3=int(df_dict[ret][3])
            dtset.append(df_dict[ret][0])
            labels.append([df_dict[ret][1],df_dict[ret][2],df_dict[ret][3]])
            try:
                ret4=dtset_dict[ret1]
            except:
                dtset_dict[ret1]={}
            try:
                ret4=dtset_dict[ret1][ret2]
            except:
                dtset_dict[ret1][ret2]={}
            try:
                dtset_dict[ret1][ret2][ret3]+=1
            except:
                dtset_dict[ret1][ret2][ret3]=1
        except:
            pass
    except:
        pass

dtset_dict

df_dict,dtset,labels,dtset_dict={},[],[],{}
for i in df:
    df_dict[i[0]]=i
for i in filenames_train:
    ret=i.split('/')[-1]
    try:
        ret1=df_dict[ret]
        try:
            ret1=df_dict[ret][1]
            ret2=df_dict[ret][2]
            ret3=df_dict[ret][3]
            try:
                try:
                    ret1=int(ret1)
                except:
                    try:
                        if int(ret2)!=3:
                            ret1=6
                            ret3=9
                        else:
                            try:
                                if int(ret3)!=9:
                                    ret1=6
                                else:
                                    continue
                            except:
                                continue
                    except:
                        try:
                            if int(ret3)!=9:
                                ret1=6
                                ret2=3
                            else:
                                continue
                        except:
                            continue
                ret4=dtset_dict[ret1]
            except:
                dtset_dict[ret1]={}
            try:
                try:
                    ret2=int(ret2)
                except:
                    try:
                        if int(ret1)!=6:
                            ret2=3
                            ret3=9
                        else:
                            try:
                                if int(ret3)!=9:
                                    ret2=3
                                else:
                                    continue
                            except:
                                continue
                    except:
                        continue
                ret4=dtset_dict[ret1][ret2]
            except:
                dtset_dict[ret1][ret2]={}
            try:
                try:
                    ret3=int(ret3)
                except:
                    try:
                        if int(ret1)!=6:
                            ret3=9
                            ret2=3
                        else:
                            try:
                                if int(ret2)!=3:
                                    ret3=9
                                else:
                                    continue
                            except:
                                continue
                    except:
                        continue
                dtset_dict[ret1][ret2][ret3]+=1
            except:
                dtset_dict[ret1][ret2][ret3]=1
            dtset.append(df_dict[ret][0])
            labels.append([ret1,ret2,ret3])
        except:
            pass
    except:
        pass

dtset_dict

X_train, X_test, y_train, y_test = train_test_split(dtset, labels, test_size=0.1, random_state=42)

class FlixStockDataset(Dataset):
    def __init__(self, filenames, label_dict, transform=None):#, target_transform=None):
        self.transform = transform
        # self.target_transform = target_transform
        self.filenames=filenames
        self.label_dict=label_dict

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        image = Image.open('/content/drive/My Drive/Colab Notebooks/FlixStock/classification-assignment.zip (Unzipped Files)/classification-assignment/images/'+self.filenames[idx])
        image=image.convert('RGB')
        label=self.label_dict[idx]#[1:]
        if self.transform:
            sample = self.transform(image)
        label = np.array([label])
        # print(label)
        label = label.astype('int64').reshape(-1, 3)
        labels = torch.tensor([label[0][0],label[0][1]+7,label[0][2]+11]).unsqueeze(0)
        target = torch.zeros(labels.size(0), 21).scatter_(1, labels, 1.)
        dict_data = {
            'img': sample,
            'labels': target[0]
            # {
            #     'pattern': label[0][0],
            #     'sleeve_length': label[0][1],
            #     'neck_type': label[0][2]
            # }
        }
        return dict_data, idx

transform = transforms.Compose([transforms.Resize((300,225)),
                                transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                  std=[0.229, 0.224, 0.225])])

transform_train = transforms.Compose([
    transforms.Resize((300,225)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=.5, hue=.5, saturation=.5, contrast=.5),
    transforms.RandomRotation(20),
    transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.2),
                            shear=None, fill=(255, 255, 255)),
    transforms.RandomPerspective(distortion_scale=0.5),
    transforms.GaussianBlur(5),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225])
])


transform_label = transforms.Compose([transforms.ToTensor()])

train_dataset = FlixStockDataset(X_train, y_train, transform=transform_train)#, target_transform=transform_label)
test_dataset = FlixStockDataset(X_test, y_test, transform=transform)#, target_transform=transform_label)

train_loader = DataLoader(train_dataset, batch_size=16,
                        shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=16,
                        shuffle=False, num_workers=0)

class_weights={'pattern':[1/i for i in np.bincount(np.array(y_train,dtype='int64')[:,0])], 'sleeve_length':[1/i for i in np.bincount(np.array(y_train,dtype='int64')[:,1])], 'neck_type':[1/i for i in np.bincount(np.array(y_train,dtype='int64')[:,2])]}

import torchvision.models as models
class MultiOutputModel(nn.Module):
    def __init__(self, patterns):
        super().__init__()
        # self.base_model = models.vgg16_bn(pretrained=True)
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.pattern = nn.Sequential(
            nn.Dropout(p=0.2),
            nn.Linear(in_features=61056, out_features=1000),
            nn.Dropout(p=0.2),
            # nn.Linear(in_features=6000, out_features=1000),
            nn.Linear(in_features=1000, out_features=patterns)
        )

    def forward(self, x):
        # for param in self.base_model.features.parameters():
        #     param.require_grad = False
        # print(x.shape)
        x = self.pool(F.tanh(self.conv1(x)))
        x = self.pool(F.tanh(self.conv2(x)))
        # print(x.shape)
        # x = self.pool(x)
        x = torch.flatten(x, start_dim=1)
        # print(self.pattern(x).shape)
        return self.pattern(x)


    # def get_loss(self, net_output, ground_truth, class_weights):#gamma=2, alpha=0.25):
    #     pattern_loss = F.cross_entropy(net_output['pattern'], ground_truth['pattern'].long(), torch.FloatTensor(class_weights['pattern']))
    #     sleeve_length_loss = F.cross_entropy(net_output['sleeve_length'], ground_truth['sleeve_length'].long(), torch.FloatTensor(class_weights['sleeve_length']))
    #     neck_type_loss = F.cross_entropy(net_output['neck_type'], ground_truth['neck_type'].long(), torch.FloatTensor(class_weights['neck_type']))
    #     # focal_loss1 = (alpha * (1-torch.exp(-pattern_loss))**gamma * pattern_loss).mean()
    #     # focal_loss2 = (alpha * (1-torch.exp(-sleeve_length_loss))**gamma * sleeve_length_loss).mean()
    #     # focal_loss3 = (alpha * (1-torch.exp(-neck_type_loss))**gamma * neck_type_loss).mean()
    #     loss = pattern_loss + sleeve_length_loss + neck_type_loss
    #     return loss, {'pattern': pattern_loss, 'sleeve_length': sleeve_length_loss, 'neck_type': neck_type_loss}

N_epochs = 50
batch_size = 16
 
model = MultiOutputModel(patterns=21)#, sleeve_types=4, neck_types=10)
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss()

from sklearn.metrics import confusion_matrix
import pandas as pd
import seaborn as sns
max_acc1,max_acc2,max_acc3 = 0,0,0
for epoch in range(10):
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        img_dict, index = data
        optimizer.zero_grad()
        outputs = model(img_dict['img'])
        loss = criterion(outputs, img_dict['labels'])#model.get_loss(outputs, img_dict['labels'])
        loss.backward()
        # loss_dict['sleeve_length'].backward()
        # loss_dict['neck_type'].backward()
        optimizer.step()
        running_loss += loss.item()
    print('Epoch[%d] loss: %.3f' % (epoch + 1, running_loss))

    pred_patt,lab_patt=[],[]
    pred_sleeve,lab_sleeve=[],[]
    pred_neck,lab_neck=[],[]
    with torch.no_grad():
        for data in test_loader:
            img_dict, index = data
            outputs = torch.sigmoid(model(img_dict['img']))
            outputs[outputs >= 0.5] = 1
            outputs[outputs < 0.5] = 0
            for j in range(len(outputs)):
                pred_patt.extend([outputs[j][:7][i].item() for i in range(len(outputs[j][:7])) if img_dict['labels'][j][:7][i].item()==1])
                lab_patt.extend([img_dict['labels'][j][:7][i].item() for i in range(len(outputs[j][:7])) if img_dict['labels'][j][:7][i].item()==1])
                pred_sleeve.extend([outputs[j][7:11][i].item() for i in range(len(outputs[j][7:11])) if img_dict['labels'][j][7:11][i].item()==1])
                lab_sleeve.extend([img_dict['labels'][j][7:11][i].item() for i in range(len(outputs[j][7:11])) if img_dict['labels'][j][7:11][i].item()==1])
                pred_neck.extend([outputs[j][11:][i].item() for i in range(len(outputs[j][11:])) if img_dict['labels'][j][11:][i].item()==1])
                lab_neck.extend([img_dict['labels'][j][11:][i].item() for i in range(len(outputs[j][11:])) if img_dict['labels'][j][11:][i].item()==1])
    print('Accuracy of the network on the test images (pattern): %.2d %%' % (accuracy_score(lab_patt,pred_patt)*100))
    print('Accuracy of the network on the test images (sleeve_type): %.2d %%' % (accuracy_score(lab_sleeve,pred_sleeve)*100))
    print('Accuracy of the network on the test images (neck_type): %.2d %%' % (accuracy_score(lab_neck,pred_neck)*100))
    # print(classification_report(lab_patt, pred_patt))
    # print(classification_report(lab_sleeve, pred_sleeve))
    # print(classification_report(lab_neck, pred_neck))
    
    pred_patt1,lab_patt1=[],[]
    pred_sleeve1,lab_sleeve1=[],[]
    pred_neck1,lab_neck1=[],[]
    with torch.no_grad():
        for data in train_loader:
            img_dict, index = data
            outputs = torch.sigmoid(model(img_dict['img']))
            outputs[outputs >= 0.5] = 1
            outputs[outputs < 0.5] = 0
            for j in range(len(outputs)):
                pred_patt1.extend([outputs[j][:7][i].item() for i in range(len(outputs[j][:7])) if img_dict['labels'][j][:7][i].item()==1])
                lab_patt1.extend([img_dict['labels'][j][:7][i].item() for i in range(len(outputs[j][:7])) if img_dict['labels'][j][:7][i].item()==1])
                pred_sleeve1.extend([outputs[j][7:11][i].item() for i in range(len(outputs[j][7:11])) if img_dict['labels'][j][7:11][i].item()==1])
                lab_sleeve1.extend([img_dict['labels'][j][7:11][i].item() for i in range(len(outputs[j][7:11])) if img_dict['labels'][j][7:11][i].item()==1])
                pred_neck1.extend([outputs[j][11:][i].item() for i in range(len(outputs[j][11:])) if img_dict['labels'][j][11:][i].item()==1])
                lab_neck1.extend([img_dict['labels'][j][11:][i].item() for i in range(len(outputs[j][11:])) if img_dict['labels'][j][11:][i].item()==1])
    print('Accuracy of the network on the train images (pattern): %.2d %%' % (accuracy_score(lab_patt1,pred_patt1)*100))
    print('Accuracy of the network on the train images (sleeve_type): %.2d %%' % (accuracy_score(lab_sleeve1,pred_sleeve1)*100))
    print('Accuracy of the network on the train images (neck_type): %.2d %%' % (accuracy_score(lab_neck1,pred_neck1)*100))
    # print(classification_report(lab_patt, pred_patt))
    # print(classification_report(lab_sleeve, pred_sleeve))
    # print(classification_report(lab_neck, pred_neck))
    if ((accuracy_score(lab_patt,pred_patt)*100) > max_acc1 and (accuracy_score(lab_patt1,pred_patt1)*100) - (accuracy_score(lab_patt,pred_patt)*100) <= 20) or ((accuracy_score(lab_sleeve,pred_sleeve)*100) > max_acc2 and (accuracy_score(lab_sleeve1,pred_sleeve1)*100) - (accuracy_score(lab_sleeve,pred_sleeve)*100) <= 20) or ((accuracy_score(lab_neck,pred_neck)*100) > max_acc1 and (accuracy_score(lab_neck1,pred_neck1)*100) - (accuracy_score(lab_neck,pred_neck)*100) <= 20):
        max_acc1=accuracy_score(lab_patt,pred_patt)*100
        max_acc2=accuracy_score(lab_sleeve,pred_sleeve)*100
        max_acc3=accuracy_score(lab_neck,pred_neck)*100
        torch.save(model.state_dict(), '/content/drive/My Drive/Colab Notebooks/FlixStock/classification-assignment.zip (Unzipped Files)/classification-assignment/Multilabel.pth')

